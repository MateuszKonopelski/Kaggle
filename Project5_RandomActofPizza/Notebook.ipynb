{
  "cells": [
    {
      "metadata": {
        "_uuid": "3cde4af100c137f080b945e3d3f9b81ad8c95607"
      },
      "cell_type": "markdown",
      "source": "# Random Act of Pizza\n\n### Description\n\n### Data Description\nSee, fork, and run a random forest benchmark model through Kaggle Scripts\n\nThis dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\n\nEach JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\n\nData fields\n\"giver_username_if_known\": Reddit username of giver if known, i.e. the person satisfying the request (\"N/A\" otherwise).\n\n\"number_of_downvotes_of_request_at_retrieval\": Number of downvotes at the time the request was collected.\n\n\"number_of_upvotes_of_request_at_retrieval\": Number of upvotes at the time the request was collected.\n\n\"post_was_edited\": Boolean indicating whether this post was edited (from Reddit).\n\n\"request_id\": Identifier of the post on Reddit, e.g. \"t3_w5491\".\n\n\"request_number_of_comments_at_retrieval\": Number of comments for the request at time of retrieval.\n\n\"request_text\": Full text of the request.\n\n\"request_text_edit_aware\": Edit aware version of \"request_text\". We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".\n\n\"request_title\": Title of the request.\n\n\"requester_account_age_in_days_at_request\": Account age of requester in days at time of request.\n\n\"requester_account_age_in_days_at_retrieval\": Account age of requester in days at time of retrieval.\n\n\"requester_days_since_first_post_on_raop_at_request\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\n\n\"requester_days_since_first_post_on_raop_at_retrieval\": Number of days between requesters first post on RAOP and time of retrieval.\n\n\"requester_number_of_comments_at_request\": Total number of comments on Reddit by requester at time of request.\n\n\"requester_number_of_comments_at_retrieval\": Total number of comments on Reddit by requester at time of retrieval.\n\n\"requester_number_of_comments_in_raop_at_request\": Total number of comments in RAOP by requester at time of request.\n\n\"requester_number_of_comments_in_raop_at_retrieval\": Total number of comments in RAOP by requester at time of retrieval.\n\n\"requester_number_of_posts_at_request\": Total number of posts on Reddit by requester at time of request.\n\n\"requester_number_of_posts_at_retrieval\": Total number of posts on Reddit by requester at time of retrieval.\n\n\"requester_number_of_posts_on_raop_at_request\": Total number of posts in RAOP by requester at time of request.\n\n\"requester_number_of_posts_on_raop_at_retrieval\": Total number of posts in RAOP by requester at time of retrieval.\n\n\"requester_number_of_subreddits_at_request\": The number of subreddits in which the author had already posted in at the time of request.\n\n\"requester_received_pizza\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\n\n\"requester_subreddits_at_request\": The list of subreddits in which the author had already posted in at the time of request.\n\n\"requester_upvotes_minus_downvotes_at_request\": Difference of total upvotes and total downvotes of requester at time of request.\n\n\"requester_upvotes_minus_downvotes_at_retrieval\": Difference of total upvotes and total downvotes of requester at time of retrieval.\n\n\"requester_upvotes_plus_downvotes_at_request\": Sum of total upvotes and total downvotes of requester at time of request.\n\n\"requester_upvotes_plus_downvotes_at_retrieval\": Sum of total upvotes and total downvotes of requester at time of retrieval.\n\n\"requester_user_flair\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (pizza given after having received, N=83).\n\n\"requester_username\": Reddit username of requester.\n\n\"unix_timestamp_of_request\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\n\n\"unix_timestamp_of_request_utc\": Unit timestamp of request in UTC."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import libries\nimport pandas as pd\nimport nltk\nimport matplotlib\n%matplotlib inline\n# import data\ndb = pd.read_json('../input/train.json')",
      "execution_count": 81,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75fd2758f88b74403586bed3800d4b83139fe505",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sh = db.shape\n\nprint('Summary:\\n', '\\tshape: {}'.format(sh))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc3c6c9e0892e5760307d4528aa87c476067efe0"
      },
      "cell_type": "markdown",
      "source": "Let's print top 5 text in for those who received top upvote "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d54d06cbee8d825484aeb73df319c70b15b22979",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "top5 = db.query('requester_received_pizza == True').sort_values(by='number_of_upvotes_of_request_at_retrieval', ascending=False)\ntop5 = top5.head(10).loc[: , ['request_title', 'request_text']]\n\nfor row in range(5):  \n    print(top5.iloc[row, 0],'\\n')\n    print(top5.iloc[row, 1], '\\n', '---'* 50)",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "645707f68753333e4eff2dbaff33afab5e78e56c"
      },
      "cell_type": "markdown",
      "source": "It looks like both request Title and request Text includes some valuable information. It could be good idea to join both those fields. "
    },
    {
      "metadata": {
        "_uuid": "d6932228fda6707c80ceee8d267f0f3adb68dc66"
      },
      "cell_type": "markdown",
      "source": "## Classification approach\nThe Random Act of Pizza db includes many information that could be useful for this challange, however, for my learning purpose (learning natural text processing) I will use only request Text and request Title combined together. "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d7d0a39a33ff7af04061f60e6d9c98d7fd4afb5c"
      },
      "cell_type": "code",
      "source": "# join both fields\ndb['text'] = db['request_title'] + ' \\n'+  db['request_text']\nlabel = db['requester_received_pizza']\ntrain = db['text']\n# convert True/False to 1/0\nlabel = label * 1",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db944de21a789c1f8f09b6312667cfea2e848e08",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2015da6977d45d85171095ffb56a2267e50e92a"
      },
      "cell_type": "code",
      "source": "ps = PorterStemmer()\ntrain_words = pd.Series()\n\nfor row in train.head(2):\n        words = word_tokenize(row)\n        for w in words[:30]: \n            st = ps.stem(word=w)\n            if w != st:\n                print(w, ': ', st)\n",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Request :  request\nColorado :  colorado\nSprings :  spring\nHelp :  help\nPlease :  pleas\nmilitary :  militari\nfamily :  famili\nhas :  ha\nreally :  realli\ntimes :  time\nRequest :  request\nCalifornia :  california\ngas :  ga\nThursday :  thursday\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "aafdb73d1ec7b275bb374d69c59fba9ea9e4a536"
      },
      "cell_type": "markdown",
      "source": "Chunking is like grouping similar words based on regular expressions we created. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2862a8bacb5d525baeed7c340724aba9942cbd7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk import RegexpParser\n\nfor row in train.head(3):\n        words = word_tokenize(row)\n        taged = pos_tag(words)\n        \n        chunkGram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n        chunkparser = RegexpParser(chunkGram)\n        chunked = chunkparser.parse(taged)\n        print(chunked)",
      "execution_count": 85,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "67020e7c7eec7263e921a87ff599244c39b3a809"
      },
      "cell_type": "markdown",
      "source": "Named Entity is looking for pre-set type of words like: organisation, people, money etc. \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5ac949de524af4cddf00e71a787fac0244166e7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk import ne_chunk\n\nfor row in train.head(3):\n        words = word_tokenize(row)\n        taged = pos_tag(words)\n        \n        namedEnt = ne_chunk(taged, binary=True)\n        print(namedEnt)",
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1dd26c9d99cee5db33775f4a81edee1fd9ce817"
      },
      "cell_type": "markdown",
      "source": "Lemitizing is similar to stem but it gives a real world not just cut version."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39a493718a7fad6df9a6a61f7a184bb6ad144beb"
      },
      "cell_type": "code",
      "source": "from nltk import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfor row in train.head(5):\n        words = word_tokenize(row)\n        for w in words:\n            lem = lemmatizer.lemmatize(w)\n            if w != lem:\n                print(w, \": \", lem)",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": "children :  child\nhas :  ha\ntimes :  time\nmeans :  mean\nwas :  wa\nhas :  ha\nus :  u\nleftovers :  leftover\nleftovers :  leftover\nguys :  guy\nstories :  story\nwas :  wa\nlives :  life\nschedules :  schedule\nus :  u\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "638bf5a3e937c0b79370758a88eb15f8ea6aea52"
      },
      "cell_type": "markdown",
      "source": "This is not useful yet as an idea but follwing code will look for synonims and antonims of a word"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "908679dcdd821cb1744eb14624066d3f2d3c8244"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import wordnet\n\nsync = wordnet.synsets('plan')\n# how to find just this word\nsync[0].lemmas()[0].name()\n# create list of synonims and antonyms\nsynonims = []\nantonims = []\nfor sync in wordnet.synsets('good'):\n    for l in sync.lemmas():\n        synonims.append(l.name())\n        if l.antonyms():\n            antonims.append(l.antonyms()[0].name())\n            \nprint(set(synonims))\nprint(set(antonims))",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'beneficial', 'secure', 'salutary', 'in_force', 'undecomposed', 'thoroughly', 'soundly', 'sound', 'honest', 'serious', 'skillful', 'proficient', 'honorable', 'adept', 'trade_good', 'estimable', 'dear', 'near', 'commodity', 'respectable', 'dependable', 'good', 'right', 'well', 'unspoiled', 'unspoilt', 'upright', 'practiced', 'safe', 'just', 'skilful', 'goodness', 'expert', 'in_effect', 'effective', 'full', 'ripe'}\n{'evilness', 'badness', 'ill', 'evil', 'bad'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07c04964ea53a6c3dc0102f3eebd435ac36f8d29"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 104,
          "data": {
            "text/plain": "'plan'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "65bfc1a73280c3c5540675f0b451f505f181a01b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e4c635e0c9a540d348b59cdf396e9355fecc4834"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f33da0698f2a753564f1b3a392246723b2de9522"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8d98a69ad19a5e9651f95424857b68ebb8fb706d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "78b4a1635129f9b47c1b0363519ecd3c620a7022"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5fb7f49cdb1e7c78132231778680fb0a4b312253"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}