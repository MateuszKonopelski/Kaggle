{
  "cells": [
    {
      "metadata": {
        "_uuid": "3cde4af100c137f080b945e3d3f9b81ad8c95607"
      },
      "cell_type": "markdown",
      "source": "# Random Act of Pizza\n\n### Description\n\nIn machine learning, it is often said there are no free lunches. How wrong we were.\n\nThis competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n\n\"I'll write a poem, sing a song, do a dance, play an instrument, whatever! I just want a pizza,\" says one hopeful poster. What about making an algorithm?\n\n\n### Data Description\nSee, fork, and run a random forest benchmark model through Kaggle Scripts\n\nThis dataset includes 5671 requests collected from the Reddit community Random Acts of Pizza between December 8, 2010 and September 29, 2013 (retrieved on September 30, 2013). All requests ask for the same thing: a free pizza. The outcome of each request -- whether its author received a pizza or not -- is known. Meta-data includes information such as: time of the request, activity of the requester, community-age of the requester, etc.\n\nEach JSON entry corresponds to one request (the first and only request by the requester on Random Acts of Pizza). We have removed fields from the test set which would not be available at the time of posting.\n\nData fields\n\"giver_username_if_known\": Reddit username of giver if known, i.e. the person satisfying the request (\"N/A\" otherwise).\n\n\"number_of_downvotes_of_request_at_retrieval\": Number of downvotes at the time the request was collected.\n\n\"number_of_upvotes_of_request_at_retrieval\": Number of upvotes at the time the request was collected.\n\n\"post_was_edited\": Boolean indicating whether this post was edited (from Reddit).\n\n\"request_id\": Identifier of the post on Reddit, e.g. \"t3_w5491\".\n\n\"request_number_of_comments_at_retrieval\": Number of comments for the request at time of retrieval.\n\n\"request_text\": Full text of the request.\n\n\"request_text_edit_aware\": Edit aware version of \"request_text\". We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".\n\n\"request_title\": Title of the request.\n\n\"requester_account_age_in_days_at_request\": Account age of requester in days at time of request.\n\n\"requester_account_age_in_days_at_retrieval\": Account age of requester in days at time of retrieval.\n\n\"requester_days_since_first_post_on_raop_at_request\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\n\n\"requester_days_since_first_post_on_raop_at_retrieval\": Number of days between requesters first post on RAOP and time of retrieval.\n\n\"requester_number_of_comments_at_request\": Total number of comments on Reddit by requester at time of request.\n\n\"requester_number_of_comments_at_retrieval\": Total number of comments on Reddit by requester at time of retrieval.\n\n\"requester_number_of_comments_in_raop_at_request\": Total number of comments in RAOP by requester at time of request.\n\n\"requester_number_of_comments_in_raop_at_retrieval\": Total number of comments in RAOP by requester at time of retrieval.\n\n\"requester_number_of_posts_at_request\": Total number of posts on Reddit by requester at time of request.\n\n\"requester_number_of_posts_at_retrieval\": Total number of posts on Reddit by requester at time of retrieval.\n\n\"requester_number_of_posts_on_raop_at_request\": Total number of posts in RAOP by requester at time of request.\n\n\"requester_number_of_posts_on_raop_at_retrieval\": Total number of posts in RAOP by requester at time of retrieval.\n\n\"requester_number_of_subreddits_at_request\": The number of subreddits in which the author had already posted in at the time of request.\n\n\"requester_received_pizza\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\n\n\"requester_subreddits_at_request\": The list of subreddits in which the author had already posted in at the time of request.\n\n\"requester_upvotes_minus_downvotes_at_request\": Difference of total upvotes and total downvotes of requester at time of request.\n\n\"requester_upvotes_minus_downvotes_at_retrieval\": Difference of total upvotes and total downvotes of requester at time of retrieval.\n\n\"requester_upvotes_plus_downvotes_at_request\": Sum of total upvotes and total downvotes of requester at time of request.\n\n\"requester_upvotes_plus_downvotes_at_retrieval\": Sum of total upvotes and total downvotes of requester at time of retrieval.\n\n\"requester_user_flair\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (pizza given after having received, N=83).\n\n\"requester_username\": Reddit username of requester.\n\n\"unix_timestamp_of_request\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\n\n\"unix_timestamp_of_request_utc\": Unit timestamp of request in UTC."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# import basic libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib\n%matplotlib inline\n\n# do not show warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import NLTK libraries\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk import RegexpParser\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk import FreqDist\n\nfrom nltk.classify.scikitlearn import SklearnClassifier\n\n# import Machine Learning Libaries\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn import cross_validation\n\n\n# IMPORT DATA\ndb = pd.read_json('../input/train.json')\ndb_test = pd.read_json('../input/test.json')",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75fd2758f88b74403586bed3800d4b83139fe505"
      },
      "cell_type": "code",
      "source": "print('Summary:\\n', '\\ttrain db shape: {}\\n\\ttest db shape: {}'.format(db.shape, db_test.shape))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Summary:\n \ttrain db shape: (4040, 32)\n\ttest db shape: (1631, 17)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fc3c6c9e0892e5760307d4528aa87c476067efe0"
      },
      "cell_type": "markdown",
      "source": "Let's print top 5 text in for those who received top upvote "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d54d06cbee8d825484aeb73df319c70b15b22979"
      },
      "cell_type": "code",
      "source": "top5 = db.query('requester_received_pizza == True').sort_values(by='number_of_upvotes_of_request_at_retrieval', ascending=False)\ntop5 = top5.head(10).loc[: , ['request_title', 'request_text']]\n\nfor row in range(5):  \n    print(top5.iloc[row, 0],'\\n')\n    print(top5.iloc[row, 1], '\\n', '---'* 50)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[Request]Vancouver, BC, Canada Father of 5, wife just got out of surgery, we were suddenly cut off from employment insurance. \n\nThe government screwed up and now we have to wait over a month for them to refile and reestablish my claim.  There is no way to expedite this at all, in spite of the fact that it is their mistake.  We have 2 girls (9 and 7) and 3 boys (5,3, 2months).\n\nMy wife had to be taken by ambulance to the hospital last week for emergency gall bladder removal surgery and we are feeling a bit beat on at the moment.  This would be a humungous pick-us-up.\n\nI am happy to provide any verification you need.  Thanks in advance.\n\n**EDIT: http://imgur.com/4FXXT Thank you so much Gama-Go!** \n ------------------------------------------------------------------------------------------------------------------------------------------------------\n[REQUEST] No sob story, it's just my birthday tomorrow and I really like pizza :D \n\nMy husband has to work 7-5 and class from 530-930 on my bday, so I just wanna order some pizza and read a book to occupy myself tomorrow. Anyone wanna hook it up with a pizza?  \n ------------------------------------------------------------------------------------------------------------------------------------------------------\n[REQUEST] Just found out about random acts of pizza. Im single father raising 2 kids and im unemployed but looking. So far our current plan for tonight is cereal again. \n\n \n ------------------------------------------------------------------------------------------------------------------------------------------------------\n[Request] Got overzealous paying off the car last week, have $1.36 until May 15. But the car no longer belongs to the bank!  \n\nThanks for getting this far. \n\nNot to pile on the bad news, because it really isn't that bad, but I just got a call from the landlord that the power's been out for a few hours in my complex, and to be careful of the food in the fridge when I get back from work. \n\nSounds like it could be a long Monday. \n~Nashville TN \n ------------------------------------------------------------------------------------------------------------------------------------------------------\n[REQUEST] [UK] GF just gave me HIV \n\nHey reddit, ive just been diagnosed as HIV positive thanks to my prostitute of a GF, and i mean actual prostitute. I was stupid enough to stay with her once i found out, she gave me a list of promises which i bought into and im paying the price. I have had to drop out of my medicine degree at uni thanks to the diagnosis and my friends are already treating me differently like an invalid - my best friends are all out tonight and havent invited me and think i dont know. I have no career, a huge student debt and a bottle of vodka to keep me company tonight. Would really appreciate a nice pizza before i have to face what happend with my life. If anyone can help please do.\n\nthanks reddit and sorry about my whinging \n ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "645707f68753333e4eff2dbaff33afab5e78e56c"
      },
      "cell_type": "markdown",
      "source": "It looks like both request Title and request Text includes some valuable information. It could be good idea to join both those fields. "
    },
    {
      "metadata": {
        "_uuid": "d6932228fda6707c80ceee8d267f0f3adb68dc66"
      },
      "cell_type": "markdown",
      "source": "## Classification approach\nThe Random Act of Pizza db includes many information that could be useful for this challange, however, for my learning purpose (learning natural text processing) I will use only request Text and request Title combined together. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7d0a39a33ff7af04061f60e6d9c98d7fd4afb5c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# join both fields\ndb['text'] = db['request_title'] + ' \\n'+  db['request_text_edit_aware']\ndb_test['text'] = db_test['request_title'] + ' \\n'+  db_test['request_text_edit_aware']\nlabel = db['requester_received_pizza']\ntrain = db['text']\ntest = db_test['text']\n# convert True/False to 1/0\nlabel = label * 1",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2015da6977d45d85171095ffb56a2267e50e92a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ps = PorterStemmer()\ntrain_words = pd.Series()\n\nfor row in train.head(2):\n        words = word_tokenize(row)\n        for w in words[:30]: \n            st = ps.stem(word=w)\n            if w != st:\n                print(w, ': ', st)\n",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aafdb73d1ec7b275bb374d69c59fba9ea9e4a536"
      },
      "cell_type": "markdown",
      "source": "Chunking is like grouping similar words based on regular expressions we created. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2862a8bacb5d525baeed7c340724aba9942cbd7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for row in train.head(3):\n        words = word_tokenize(row)\n        taged = pos_tag(words)\n        \n        chunkGram = r'''Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'''\n        chunkparser = RegexpParser(chunkGram)\n        chunked = chunkparser.parse(taged)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "67020e7c7eec7263e921a87ff599244c39b3a809"
      },
      "cell_type": "markdown",
      "source": "Named Entity is looking for pre-set type of words like: organisation, people, money etc. \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5ac949de524af4cddf00e71a787fac0244166e7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk import ne_chunk\n\nfor row in train.head(3):\n        words = word_tokenize(row)\n        taged = pos_tag(words)\n        \n        namedEnt = ne_chunk(taged, binary=True)",
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1dd26c9d99cee5db33775f4a81edee1fd9ce817"
      },
      "cell_type": "markdown",
      "source": "Lemitizing is similar to stem but it gives a real world not just cut version."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39a493718a7fad6df9a6a61f7a184bb6ad144beb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "lemmatizer = WordNetLemmatizer()\n\nfor row in train.head(5):\n        words = word_tokenize(row)\n        for w in words:\n            lem = lemmatizer.lemmatize(w)\n            if w != lem:\n                print(w, \": \", lem)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "638bf5a3e937c0b79370758a88eb15f8ea6aea52"
      },
      "cell_type": "markdown",
      "source": "This is not useful yet as an idea but follwing code will look for synonims and antonims of a word"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "908679dcdd821cb1744eb14624066d3f2d3c8244",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sync = wordnet.synsets('plan')\n# how to find just this word\nsync[0].lemmas()[0].name()\n# create list of synonims and antonyms\nsynonims = []\nantonims = []\nfor sync in wordnet.synsets('good'):\n    for l in sync.lemmas():\n        synonims.append(l.name())\n        if l.antonyms():\n            antonims.append(l.antonyms()[0].name())\n            \nprint(set(synonims))\nprint(set(antonims))",
      "execution_count": 107,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2490e53be76d1719a99ba8d9bd9530241190f017"
      },
      "cell_type": "markdown",
      "source": "## Data preparation\nBefore we can build any model we need to pre=process our data.\nThe train and test data is now saved in array of text. We need to to convert into format that will be readible by NTLK. \n\nTo do so, we run following steps:\n1. tokenized text into separate words.\n* Find 3000 most common words \n* convert each text to see, whether it includes those 3000 texts\n* Build a featuresets list including tuples: ({feature: True/False}, label)\n\n### Build function to extract features"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6909eec5fc36342e7e073fc6079dc55ec61f6f67"
      },
      "cell_type": "code",
      "source": "# Create list of all words used:\nall_words = []\nfor row in train:\n    all_words.extend(word_tokenize(row.lower()))\n\nall_words = nltk.FreqDist(all_words)\n\n# Create 1000 categories from all 15.000 words. 1000 is obj number and I might increase it in future. \nword_features = [w[0] for w in all_words.most_common(3000)]\n\n# create function that would create a list of feeatures from word_features list\ndef find_features(list_of_words):\n    unique_list_of_words = set(list_of_words)\n    features = {}\n    for w in word_features:\n        features[w] = (w in unique_list_of_words)\n    return features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07c04964ea53a6c3dc0102f3eebd435ac36f8d29",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Extract features on train set. "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "95f6890eea043e723176b1c01b7b3edbdf1a6870"
      },
      "cell_type": "code",
      "source": "# Create a list of tupples for each Label and features:\nrequest_tupples = []\nfor row in range(train.shape[0]):\n    request_tupples.append((word_tokenize(train[row].lower()), label[row]))\n\n# Extract Features\nfeaturesets = [(find_features(text), label) for (text, label) in request_tupples]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "678eeaee9c2eeeb3bee4e06fc56003e60126d4d4"
      },
      "cell_type": "markdown",
      "source": "## MachineLearning Algorithms\nOur training set is prepared for ml algorithms testing. \nThe procedure here is:\n1. List all most popular ML algorithms in list\n* Loop through this list and apply each of techniques to part of train set\n* Divide Train sets into 10 parts and for each loop use 90% of set to train set and 10% to test it. NTLK doesn't have Cross_validate method like sklearn therefore we need to design\n* Print the mean prediction for each method in order to choose which methods will be used for final model. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04f9a9a508cbd614b4aac30b03ce94711d9ce07b"
      },
      "cell_type": "code",
      "source": "# Run a loop through the most popular ML alg and print their results.\nalgorithms = [# NB\n              'MultinomialNB',\n              'BernoulliNB',\n              \n              # Linear models\n              'LogisticRegression',\n              'Perceptron',\n              'RidgeClassifier',\n              'SGDClassifier',\n              \n              # SVM\n              'SVC',\n              'LinearSVC',\n              \n              # Ensemble methods\n              'RandomForestClassifier',\n              'AdaBoostClassifier',\n              'BaggingClassifier',\n              'ExtraTreesClassifier',\n              'GradientBoostingClassifier',\n              \n              # Tree models\n              'DecisionTreeClassifier',\n              'ExtraTreeClassifier',\n              \n              # KNN\n              'KNeighborsClassifier']\n\nfor alg in algorithms:\n    accu = []\n    classifier = SklearnClassifier(eval(alg + '()'))\n    cv = cross_validation.KFold(len(featuresets), n_folds=10, shuffle=False, random_state=None)\n    \n    for traincv, testcv in cv:\n        classifier = classifier.train(featuresets[traincv[0]:traincv[len(traincv)-1]])\n        accu.append(nltk.classify.util.accuracy(classifier, featuresets[testcv[0]:testcv[len(testcv)-1]]))\n        \n    accu = np.mean(accu)\n    print('{} accuracy: {}%'.format(str(alg), round(accu*100, 2)))\n\nprint('-'*20,'\\nFinished')",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "MultinomialNB accuracy: 79.78%\nBernoulliNB accuracy: 70.15%\nLogisticRegression accuracy: 88.34%\nPerceptron accuracy: 74.74%\nRidgeClassifier accuracy: 88.61%\nSGDClassifier accuracy: 77.3%\nSVC accuracy: 75.38%\nLinearSVC accuracy: 91.51%\nRandomForestClassifier accuracy: 92.66%\nAdaBoostClassifier accuracy: 76.05%\nBaggingClassifier accuracy: 91.89%\nExtraTreesClassifier accuracy: 94.52%\nGradientBoostingClassifier accuracy: 78.14%\nDecisionTreeClassifier accuracy: 92.85%\nExtraTreeClassifier accuracy: 93.05%\nKNeighborsClassifier accuracy: 76.38%\n-------------------- \nFinished\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f532414e9645fa98c9b2224fa3789fb221243b66"
      },
      "cell_type": "markdown",
      "source": "## Test set preparation\nNow we need to prepare Test set in similar way to Train but simplier because the find_features function is already built. \n\n### Extract features on Test set"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f33da0698f2a753564f1b3a392246723b2de9522"
      },
      "cell_type": "code",
      "source": "request_test = []\nfor row in range(test.shape[0]):\n    request_test.append((word_tokenize(test[row].lower())))\n    \ntesting = [(find_features(text)) for (text) in request_test]",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb7aefe3e77fa7c9eb62b0ac862e5c948d420432"
      },
      "cell_type": "markdown",
      "source": "## Build prediction\nBased on algorithm search above we chose 8 methods below to build predictions. Each of them had high prediction average. And each of them will be upload on Kaggle site to see the average. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d98a69ad19a5e9651f95424857b68ebb8fb706d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# chosen algorithms:\nalgorithms = ['LogisticRegression',\n              'RidgeClassifier',\n              'LinearSVC',\n              'RandomForestClassifier',\n              'BaggingClassifier',\n              'KNeighborsClassifier']\n\nfor alg in algorithms:\n    classifier = SklearnClassifier(eval(alg + '()'))\n    classifier = classifier.train(featuresets)\n    prediction = classifier.classify_many(testing)\n\n    prediction = pd.concat([pd.Series(db_test['request_id']), pd.Series(prediction)], axis=1, ignore_index=True)\n    prediction.columns = ['request_id', 'requester_received_pizza']\n    prediction.to_csv('{}.csv'.format(alg), index=False)",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fc5a1f49f1b99686546f6839c9c13a8e4b062e05"
      },
      "cell_type": "markdown",
      "source": "Unfortunately, the result (Kaggle Score) is still much lower than I would like it to be. The best method (KNN) received a a score of 56%, which indicates that probably there is a problem with in my algorithm search method as they indicated much higher results. Also, we need to different approach here to build a better prediction. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1efd15087dc83f85695dbd39a456c9a9ade8b897"
      },
      "cell_type": "code",
      "source": "print('Finished')",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Finished\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fb7f49cdb1e7c78132231778680fb0a4b312253",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "---------------------"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "eba3258aa4ca0757a22266b4b639a7751afa03ce"
      },
      "cell_type": "markdown",
      "source": "# New approach ?? \nMaybe Keras?"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "47194ca846d8b631910045f3a932bbc61ad0d176"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}